# Оптимизация инференса

## Введение

Большие языковые модели очень популярны в наши дни, они стали state-of-the-art подходом для многих задач. Однако данные модели очень требовательны к ресурсам во время обучения и использования. Чрезвычайно высокие затраты как по памяти, так и по времени во время инференса, является серьезным препятствием для внедрения трансформеров для решения реальных задач.

Почему инференс больших языковых моделей настолько ресурсозатратен? Помимо увеличения размера моделей SoTA, есть два основных фактора [Pope et al., 2022](https://arxiv.org/abs/2211.05102):

- *Большой расход памяти*. Как параметры модели, так и промежуточные состояния необходимы в памяти во время инференса. Например,
  - Кэш KV должен храниться в памяти во время стадии декодирования; например, для размера батча 512 и длины контекста 2048 общий объем кэша KV составляет 3 ТБ, что в 3 раза превышает размер модели (!).
  - Стоимость инференса из-за механизма attention квадратично зависит от длины входной последовательности.
- *Низкая распараллеливаемость*. Генерация выполняется авторегрессионным способом, что затрудняет распараллеливание процесса декодирования.

## Обзор методов

Цели оптимизации инференса модели:

- Сокращение объема памяти модели за счет использования меньшего количества устройств с GPU и меньшего объема памяти GPU;
- Снижение сложности вычислений за счет уменьшения FLOPs;
- Уменьшение inference latency и ускорение работы.

Можно использовать несколько методов, чтобы сделать инференс менее затратным по памяти и/или быстрее по времени.

- Применяйте параллелизм для масштабирования модели на большом количестве GPU. Умный параллелизм компонентов модели и данных позволяет запускать модель с триллионами параметров.
- Выгрузки временно неиспользуемых данных в CPU и последующего считывания их обратно при необходимости. Это помогает уменьшить использование памяти, но приводит к увеличению задержки.
- Особые стратегии пакетной обработки; Например, [EffectiveTransformer](https://github.com/bytedance/effective_transformer) объединяет последовательности входных данных, для того чтобы удалять паддинги в рамках одного батча.
- Методы сжатия сети, такие как прунинг, квантизация, дистилляция. Модель меньшего размера, с точки зрения количества параметров или разрядности, должна требовать меньше памяти и работать быстрее.
- Улучшения, характерные для архитектуры целевой модели. Многие архитектурные изменения, особенно для слоёв внимания, помогают повысить скорость стадии декодирования трансформера.

## Инструменты для разработчиков

**TensorRT-LLM** предоставляет пользователям простой в использовании Python API для описания больших языковых моделей (LLM) и создания движков TensorRT, которые содержат самые современные оптимизации для эффективного выполнения инференса на графических процессорах NVIDIA. TensorRT-LLM также содержит компоненты для создания сред выполнения на Python и C++, которые выполняют эти движки TensorRT.

![Alt text](tensorrt.png)
